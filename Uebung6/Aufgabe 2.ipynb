{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5485545821317872563\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6764586599\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2465288877563383004\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from bilderGenerator import zieheBilder\n",
    "import matplotlib.pyplot as plt\n",
    "test = zieheBilder(50)\n",
    "train = zieheBilder(500)\n",
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "colors = ['red','blue','red']\n",
    "for i in range(len(test[0])):\n",
    "    plt.plot(test[0][i], test[1][i],'ro', color= colors[test[2][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123) #um die Gewichte immer gleich zufaellig zu initialisieren\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(123) # um die Gewichte immer gleichzufaellig zu initialisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.dstack(train[:2])[0]#X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = np.dstack(test[:2])[0]#X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "Y_train = np_utils.to_categorical(train[2]==1,2)\n",
    "Y_test = np_utils.to_categorical(test[2]==1,2)\n",
    "X_train = X_train.reshape(X_train.shape[0], 2, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 2, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0,\n",
    "                          write_graph=True, write_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 0s 186us/step - loss: 0.7634 - acc: 0.7100 - val_loss: 0.3683 - val_acc: 0.8750\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 0s 62us/step - loss: 0.5351 - acc: 0.7937 - val_loss: 0.2509 - val_acc: 0.9200\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 0s 60us/step - loss: 0.3239 - acc: 0.8862 - val_loss: 0.2519 - val_acc: 0.9450\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1753 - acc: 0.9350 - val_loss: 0.2555 - val_acc: 0.9200\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1201 - acc: 0.9550 - val_loss: 0.1484 - val_acc: 0.9500\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 0s 60us/step - loss: 0.0985 - acc: 0.9612 - val_loss: 0.2408 - val_acc: 0.9200\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1203 - acc: 0.9563 - val_loss: 0.1694 - val_acc: 0.9200\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 0s 60us/step - loss: 0.1125 - acc: 0.9612 - val_loss: 0.1410 - val_acc: 0.9500\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 0s 66us/step - loss: 0.1103 - acc: 0.9600 - val_loss: 0.1375 - val_acc: 0.9500\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1286 - acc: 0.9537 - val_loss: 0.1392 - val_acc: 0.9500\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1415 - acc: 0.9425 - val_loss: 0.1396 - val_acc: 0.9500\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 0s 64us/step - loss: 0.1287 - acc: 0.9525 - val_loss: 0.1576 - val_acc: 0.9500\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1618 - acc: 0.9387 - val_loss: 0.1627 - val_acc: 0.9250\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1279 - acc: 0.9550 - val_loss: 0.2691 - val_acc: 0.9050\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 0s 62us/step - loss: 0.1313 - acc: 0.9475 - val_loss: 0.1416 - val_acc: 0.9500\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1454 - acc: 0.9438 - val_loss: 0.1422 - val_acc: 0.9550\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 0s 66us/step - loss: 0.1118 - acc: 0.9550 - val_loss: 0.1672 - val_acc: 0.9200\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 0s 66us/step - loss: 0.1090 - acc: 0.9587 - val_loss: 0.1472 - val_acc: 0.9500\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1006 - acc: 0.9637 - val_loss: 0.1441 - val_acc: 0.9500\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 0s 63us/step - loss: 0.1197 - acc: 0.9600 - val_loss: 0.1553 - val_acc: 0.9200\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1026 - acc: 0.9662 - val_loss: 0.1455 - val_acc: 0.9350\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1038 - acc: 0.9600 - val_loss: 0.1488 - val_acc: 0.9400\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 0s 60us/step - loss: 0.0966 - acc: 0.9587 - val_loss: 0.1475 - val_acc: 0.9500\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 0s 62us/step - loss: 0.1020 - acc: 0.9612 - val_loss: 0.2253 - val_acc: 0.9250\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1129 - acc: 0.9625 - val_loss: 0.1382 - val_acc: 0.9500\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1082 - acc: 0.9563 - val_loss: 0.1396 - val_acc: 0.9500\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1058 - acc: 0.9587 - val_loss: 0.1407 - val_acc: 0.9450\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1380 - acc: 0.9537 - val_loss: 0.1410 - val_acc: 0.9350\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 0s 66us/step - loss: 0.1096 - acc: 0.9563 - val_loss: 0.1464 - val_acc: 0.9350\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1058 - acc: 0.9600 - val_loss: 0.1391 - val_acc: 0.9500\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 0s 63us/step - loss: 0.1000 - acc: 0.9575 - val_loss: 0.2027 - val_acc: 0.9200\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 0s 60us/step - loss: 0.0997 - acc: 0.9612 - val_loss: 0.1861 - val_acc: 0.9200\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0925 - acc: 0.9625 - val_loss: 0.1638 - val_acc: 0.9200\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 0s 60us/step - loss: 0.0955 - acc: 0.9637 - val_loss: 0.1518 - val_acc: 0.9200\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0930 - acc: 0.9625 - val_loss: 0.1990 - val_acc: 0.9200\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 0s 60us/step - loss: 0.1084 - acc: 0.9600 - val_loss: 0.1484 - val_acc: 0.9550\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 0s 68us/step - loss: 0.1061 - acc: 0.9625 - val_loss: 0.1550 - val_acc: 0.9200\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 0s 65us/step - loss: 0.1165 - acc: 0.9550 - val_loss: 0.1483 - val_acc: 0.9500\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0970 - acc: 0.9612 - val_loss: 0.1477 - val_acc: 0.9250\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 0s 62us/step - loss: 0.0995 - acc: 0.9587 - val_loss: 0.1489 - val_acc: 0.9200\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 0s 63us/step - loss: 0.0927 - acc: 0.9625 - val_loss: 0.1398 - val_acc: 0.9400\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 0s 63us/step - loss: 0.0962 - acc: 0.9587 - val_loss: 0.1484 - val_acc: 0.9250\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 0s 62us/step - loss: 0.0964 - acc: 0.9600 - val_loss: 0.1865 - val_acc: 0.9200\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 0s 62us/step - loss: 0.1076 - acc: 0.9563 - val_loss: 0.1393 - val_acc: 0.9400\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0940 - acc: 0.9600 - val_loss: 0.1434 - val_acc: 0.9500\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 0s 60us/step - loss: 0.1040 - acc: 0.9625 - val_loss: 0.1356 - val_acc: 0.9500\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1023 - acc: 0.9600 - val_loss: 0.1614 - val_acc: 0.9350\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1038 - acc: 0.9587 - val_loss: 0.1345 - val_acc: 0.9500\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 0s 66us/step - loss: 0.0974 - acc: 0.9587 - val_loss: 0.1804 - val_acc: 0.9250\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 0s 60us/step - loss: 0.0953 - acc: 0.9637 - val_loss: 0.1311 - val_acc: 0.9450\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 0s 62us/step - loss: 0.1060 - acc: 0.9575 - val_loss: 0.1461 - val_acc: 0.9250\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 0s 62us/step - loss: 0.0943 - acc: 0.9612 - val_loss: 0.1429 - val_acc: 0.9550\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1041 - acc: 0.9575 - val_loss: 0.1456 - val_acc: 0.9200\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 0s 63us/step - loss: 0.0987 - acc: 0.9637 - val_loss: 0.1371 - val_acc: 0.9400\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 0s 62us/step - loss: 0.1004 - acc: 0.9587 - val_loss: 0.1540 - val_acc: 0.9200\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1051 - acc: 0.9600 - val_loss: 0.1377 - val_acc: 0.9400\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 0s 68us/step - loss: 0.1008 - acc: 0.9587 - val_loss: 0.1708 - val_acc: 0.9200\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 0s 62us/step - loss: 0.0999 - acc: 0.9650 - val_loss: 0.1381 - val_acc: 0.9350\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0944 - acc: 0.9625 - val_loss: 0.1637 - val_acc: 0.9200\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 0s 62us/step - loss: 0.1009 - acc: 0.9637 - val_loss: 0.1452 - val_acc: 0.9500\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 0s 63us/step - loss: 0.1110 - acc: 0.9587 - val_loss: 0.1524 - val_acc: 0.9200\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1120 - acc: 0.9587 - val_loss: 0.1889 - val_acc: 0.9200\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0994 - acc: 0.9625 - val_loss: 0.1327 - val_acc: 0.9500\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0896 - acc: 0.9612 - val_loss: 0.1439 - val_acc: 0.9350\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0926 - acc: 0.9612 - val_loss: 0.1414 - val_acc: 0.9500\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0981 - acc: 0.9550 - val_loss: 0.1523 - val_acc: 0.9200\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0990 - acc: 0.9625 - val_loss: 0.1352 - val_acc: 0.9450\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0934 - acc: 0.9612 - val_loss: 0.1391 - val_acc: 0.9500\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 0s 66us/step - loss: 0.0942 - acc: 0.9625 - val_loss: 0.1446 - val_acc: 0.9400\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0928 - acc: 0.9575 - val_loss: 0.1564 - val_acc: 0.9250\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 0s 60us/step - loss: 0.1061 - acc: 0.9537 - val_loss: 0.1360 - val_acc: 0.9500\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 0s 64us/step - loss: 0.0938 - acc: 0.9625 - val_loss: 0.1373 - val_acc: 0.9500\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0961 - acc: 0.9612 - val_loss: 0.1324 - val_acc: 0.9500\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0951 - acc: 0.9625 - val_loss: 0.1373 - val_acc: 0.9400\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0937 - acc: 0.9600 - val_loss: 0.1333 - val_acc: 0.9500\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1161 - acc: 0.9600 - val_loss: 0.2496 - val_acc: 0.9200\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 0s 69us/step - loss: 0.1036 - acc: 0.9587 - val_loss: 0.1358 - val_acc: 0.9400\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 0s 62us/step - loss: 0.0951 - acc: 0.9550 - val_loss: 0.1357 - val_acc: 0.9300\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 0s 60us/step - loss: 0.0922 - acc: 0.9625 - val_loss: 0.1687 - val_acc: 0.9200\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 0s 62us/step - loss: 0.1027 - acc: 0.9600 - val_loss: 0.1292 - val_acc: 0.9500\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 0s 65us/step - loss: 0.1036 - acc: 0.9612 - val_loss: 0.1338 - val_acc: 0.9500\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1402 - acc: 0.9438 - val_loss: 0.1721 - val_acc: 0.9200\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0992 - acc: 0.9662 - val_loss: 0.1634 - val_acc: 0.9200\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0976 - acc: 0.9612 - val_loss: 0.1556 - val_acc: 0.9200\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 0s 60us/step - loss: 0.0978 - acc: 0.9650 - val_loss: 0.1869 - val_acc: 0.9200\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0984 - acc: 0.9650 - val_loss: 0.1375 - val_acc: 0.9400\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0946 - acc: 0.9600 - val_loss: 0.1410 - val_acc: 0.9350\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 0s 60us/step - loss: 0.0997 - acc: 0.9575 - val_loss: 0.1465 - val_acc: 0.9250\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 0s 66us/step - loss: 0.0893 - acc: 0.9650 - val_loss: 0.1340 - val_acc: 0.9500\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 0s 60us/step - loss: 0.0914 - acc: 0.9575 - val_loss: 0.1354 - val_acc: 0.9500\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0937 - acc: 0.9587 - val_loss: 0.1406 - val_acc: 0.9200\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0912 - acc: 0.9575 - val_loss: 0.1376 - val_acc: 0.9500\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0996 - acc: 0.9575 - val_loss: 0.1390 - val_acc: 0.9350\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.1053 - acc: 0.9575 - val_loss: 0.1576 - val_acc: 0.9250\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 0s 60us/step - loss: 0.0900 - acc: 0.9612 - val_loss: 0.1633 - val_acc: 0.9250\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 0s 60us/step - loss: 0.0926 - acc: 0.9637 - val_loss: 0.1432 - val_acc: 0.9350\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 0s 68us/step - loss: 0.0913 - acc: 0.9563 - val_loss: 0.1430 - val_acc: 0.9400\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 0s 62us/step - loss: 0.0934 - acc: 0.9637 - val_loss: 0.1449 - val_acc: 0.9350\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 0s 61us/step - loss: 0.0957 - acc: 0.9625 - val_loss: 0.1407 - val_acc: 0.9250\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 0s 63us/step - loss: 0.0898 - acc: 0.9587 - val_loss: 0.1544 - val_acc: 0.9200\n",
      "100/100 [==============================] - 0s 34us/step\n",
      "\n",
      " test acc 0.94\n",
      "{'val_loss': [0.3683368921279907, 0.25088029623031616, 0.2518654692173004, 0.25551647663116456, 0.14837164044380188, 0.240787011384964, 0.1693563675880432, 0.14100818186998368, 0.13749203830957413, 0.13923780113458634, 0.1396114456653595, 0.15755528926849366, 0.162690172791481, 0.2690998685359955, 0.14157413184642792, 0.14216386079788207, 0.1672159206867218, 0.14717506229877472, 0.14409889340400694, 0.15531850039958953, 0.14551133036613464, 0.14881250500679016, 0.14754946857690812, 0.2252855885028839, 0.13818785339593886, 0.13960774838924409, 0.1406792524456978, 0.14099262177944183, 0.14637231349945068, 0.1390722608566284, 0.20265731871128081, 0.1861239814758301, 0.16375857949256897, 0.1518301260471344, 0.19896681368350982, 0.14838119238615036, 0.15498191595077515, 0.1483250626921654, 0.14772680640220642, 0.1488830590248108, 0.13978323817253113, 0.14837915122509002, 0.18650569379329682, 0.13931505501270294, 0.1434209156036377, 0.1355666509270668, 0.16135088741779327, 0.13453976899385453, 0.18044672191143035, 0.13109060972929001, 0.1461333429813385, 0.14294539093971254, 0.14559396147727965, 0.13713604599237442, 0.15396481812000273, 0.13772502779960633, 0.17081653237342834, 0.1380759984254837, 0.16370928943157195, 0.14521352887153627, 0.1523922026157379, 0.18890114784240722, 0.13273937702178956, 0.14388713002204895, 0.14136592745780946, 0.1522906231880188, 0.13519189804792403, 0.1391400334239006, 0.14459551453590394, 0.15636100351810456, 0.1360250136256218, 0.13726555079221725, 0.13235042959451676, 0.1372656574845314, 0.13330376118421555, 0.24962297320365906, 0.13583642601966858, 0.13570429503917694, 0.16872148156166078, 0.12924351155757904, 0.1338192516565323, 0.17205725431442262, 0.16335347473621367, 0.15558673560619354, 0.18694466710090638, 0.13747020304203034, 0.14100867629051209, 0.14651265978813172, 0.13397140324115753, 0.13541863411664962, 0.14061727643013, 0.1375868284702301, 0.13896355509757996, 0.15763046383857726, 0.1632719612121582, 0.14315151154994965, 0.1429525524377823, 0.14491843402385712, 0.14071060180664063, 0.1543748837709427], 'val_acc': [0.875, 0.92, 0.945, 0.92, 0.95, 0.92, 0.92, 0.95, 0.95, 0.95, 0.95, 0.95, 0.925, 0.905, 0.95, 0.955, 0.92, 0.95, 0.95, 0.92, 0.935, 0.94, 0.95, 0.925, 0.95, 0.95, 0.945, 0.935, 0.935, 0.95, 0.92, 0.92, 0.92, 0.92, 0.92, 0.955, 0.92, 0.95, 0.925, 0.92, 0.94, 0.925, 0.92, 0.94, 0.95, 0.95, 0.935, 0.95, 0.925, 0.945, 0.925, 0.955, 0.92, 0.94, 0.92, 0.94, 0.92, 0.935, 0.92, 0.95, 0.92, 0.92, 0.95, 0.935, 0.95, 0.92, 0.945, 0.95, 0.94, 0.925, 0.95, 0.95, 0.95, 0.94, 0.95, 0.92, 0.94, 0.93, 0.92, 0.95, 0.95, 0.92, 0.92, 0.92, 0.92, 0.94, 0.935, 0.925, 0.95, 0.95, 0.92, 0.95, 0.935, 0.925, 0.925, 0.935, 0.94, 0.935, 0.925, 0.92], 'loss': [0.7633887243270874, 0.5351462054252625, 0.32392268426716325, 0.17527998268604278, 0.12008276350796222, 0.09848755169659854, 0.12030214346945285, 0.11250230588018895, 0.11032004788517952, 0.1286184872686863, 0.14154703170061111, 0.128736275434494, 0.16179294116795062, 0.12793976798653603, 0.1313243256509304, 0.14543937355279923, 0.11176961489021778, 0.10895201414823533, 0.10061362609267235, 0.11968096971511841, 0.10261943269520998, 0.10377794742584229, 0.09661916859447955, 0.10204974826425314, 0.11293641336262226, 0.10818933803588152, 0.10575394738465548, 0.1380497083067894, 0.10962650492787361, 0.1058057439699769, 0.09997949790209532, 0.09972430367022753, 0.09251884534955025, 0.09545168146491051, 0.09298512525856495, 0.10842561474069953, 0.10610235877335071, 0.11645768657326698, 0.09699458941817284, 0.09946583323180676, 0.09271366003900766, 0.09623427800834179, 0.09639083869755267, 0.10762478038668633, 0.09403920851647854, 0.10395792566239834, 0.10232001803815365, 0.10380098968744278, 0.09741813268512488, 0.09532978866249323, 0.10597879730165005, 0.09427572906017304, 0.10409809194505215, 0.09866963475942611, 0.10035682305693626, 0.10509431391954421, 0.10084193758666515, 0.09991195358335972, 0.09442996747791767, 0.10094822525978088, 0.11101899467408657, 0.11195461109280586, 0.0994450630992651, 0.0895599241554737, 0.09261212162673474, 0.09809716496616602, 0.09897860310971737, 0.09340172298252583, 0.09419248204678297, 0.0928385654464364, 0.10605545114725828, 0.09377209439873696, 0.0960738393291831, 0.09514729298651219, 0.09367312788963318, 0.11609144903719425, 0.10358718045055866, 0.09514672473073006, 0.0921616368740797, 0.10272222191095352, 0.10358388610184192, 0.14024223506450653, 0.09924199845641851, 0.09759620495140553, 0.09782259896397591, 0.09840743869543075, 0.09456210784614086, 0.09972605220973492, 0.08928665339946747, 0.09139673724770546, 0.0937009509652853, 0.09119935339316726, 0.0996447056159377, 0.10525745183229446, 0.09001292899250984, 0.09263814605772495, 0.09132149040699006, 0.0934132757410407, 0.09566102623939514, 0.08978960921987891], 'acc': [0.71, 0.79375, 0.88625, 0.935, 0.955, 0.96125, 0.95625, 0.96125, 0.96, 0.95375, 0.9425, 0.9525, 0.93875, 0.955, 0.9475, 0.94375, 0.955, 0.95875, 0.96375, 0.96, 0.96625, 0.96, 0.95875, 0.96125, 0.9625, 0.95625, 0.95875, 0.95375, 0.95625, 0.96, 0.9575, 0.96125, 0.9625, 0.96375, 0.9625, 0.96, 0.9625, 0.955, 0.96125, 0.95875, 0.9625, 0.95875, 0.96, 0.95625, 0.96, 0.9625, 0.96, 0.95875, 0.95875, 0.96375, 0.9575, 0.96125, 0.9575, 0.96375, 0.95875, 0.96, 0.95875, 0.965, 0.9625, 0.96375, 0.95875, 0.95875, 0.9625, 0.96125, 0.96125, 0.955, 0.9625, 0.96125, 0.9625, 0.9575, 0.95375, 0.9625, 0.96125, 0.9625, 0.96, 0.96, 0.95875, 0.955, 0.9625, 0.96, 0.96125, 0.94375, 0.96625, 0.96125, 0.965, 0.965, 0.96, 0.9575, 0.965, 0.9575, 0.95875, 0.9575, 0.9575, 0.9575, 0.96125, 0.96375, 0.95625, 0.96375, 0.9625, 0.95875]}\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() \n",
    " \n",
    "model.add(Flatten(input_shape=(2,1)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=0.01, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(X_train, Y_train, \n",
    "          batch_size=32, epochs=100, validation_split=0.2,verbose=1, callbacks=[tensorboard])\n",
    " \n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('\\n', 'test acc', score[1])\n",
    "\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
